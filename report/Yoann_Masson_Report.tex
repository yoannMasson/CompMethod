\documentclass[12pt, a4paper]{report}
\usepackage {graphicx,booktabs, gensymb,amsmath,appendix}
\usepackage[final]{pdfpages} 

\title{Study of multiple Heat Diffusion schemes through a C++ implementation}
\author{Yoann Masson \\ Student in Software Engineering at Cranfield University}
\date{5\textsuperscript{th} of december, 2017}

\graphicspath{{plot/}}
\begin{document}

\begin{titlepage}
\maketitle
\end{titlepage}


\begin{abstract}
This report is about studying 4 schemes to implement a numerical solution of the heat diffusion equation in a 1D material through time. The equation is given by the following formula: \begin{equation}\frac{\partial T}{\partial t} = D\frac{\partial^2T }{\partial x^2}
\end{equation} 
During this paper we will see that implicit methods are more accurate and more time consumming than explicit methods. The heat diffusion is one of the "solved" phenomena, meaning that we know how to model it and that is how we are able to determine the accuracy of a method. \\
Schemes relies on computing steps after steps that relie on each other making the time steps size a big matter in this subject. The more steps there is, the more accurate is the method, in condition that the method is stable. This matter will be treated by comparing the same schemes with differents time steps' size.


\end{abstract}


\tableofcontents

\chapter*{Introduction}
The simulation of physics phenomena is one of the many uses of modern computer. Thoses simulations are based on the exact formula or on approximations based on observations of the phenomenas' behaviour.\\
Because the phenomena often depends on many parameters, the analytic equation is not often known. So that, the only way the model a phenomena is to try to approch it with differents schemes based on observations of the initial conditions. However, simulations has very different accuracy and time computing meaning that every phenomena have to be treated separatly.\\
In this report we will deal with the heat diffusion problem with the following constraints: a one foot long wall at an initial temperature of 100\degree F is being subject to a rise of its surface temperature to 300\degree F. The diffusivity D of the material is 0.1ft\textsuperscript{2}/hr. We will study the rise of the temperature of the material with time t = { 0.0, 0.1, 0.2, 0.3, 0.4, 0.5}.
\\
We will see four different schemes, each of them is providing a numerical solution. Those schemes are split into two categories:
\begin{itemize}

\item explicit schemes, schemes where there is only one unknown in the equation making it eazier to implement but less acurate
\begin{itemize}
\item DuFort \& Frankel Method
\item Richardson Method
\end{itemize}
\item implicit schemes, schemes where there are mutiple unkowns that needs to be solved through a linear system, which is harder and more time consuming to solve, but are far more accurate.
\begin{itemize}
\item Laasonen Method
\item Crank \& Nicolson Method
\end{itemize}
\end{itemize}

The four methods will give four different solutions that we will compare with the known analytical solution. Firstly the discussion will be oriented toward the behaviour of the errors and secondly we will discuss about the C++ solution that I provided.
\chapter{The analytical solution}
\section{The expected behaviour}
By chance, the heat diffusion equation has been solved and we know the temperature at the given time \textit{t} and space \textit{x}, with the given formula: \begin{equation}
T = T_{sur} + 2(T_{in} - T_{sur})\sum_{1}^{\infty}e^{-D(m\pi/L)^{2}t}\frac{1-(-1)^{m}}{m\pi}sin(\frac{m\pi x}{L})
\end{equation}
If we  plot the result for the wanted times, which are from time = 0 to time = 0.5 with a 0.1 time step we get the following plot:
\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{Analytic.png}
\caption{Analytical solution}
\end{figure}
\\
On the horizontale axis is the space and on the vertical is the temperature. Each line represents the temperature of the wall at a given time.
As expected as the time flows the temperature is rising from the surface to the center. This is usefull to understand the heat diffusion behaviour. We will later use the data, used to make this plot, to compare with the other solutions. This plot has been computed with a \textit{m} value of 50, which we will discuss in the next section.


\section{Study of \textit{m}}
\paragraph{}
\begin{equation}
\sum_{1}^{\infty}e^{-D(m\pi/L)^{2}t}\frac{1-(-1)^{m}}{m\pi}sin(\frac{m\pi x}{L})
\end{equation}
When considering the analytical equation of the heat diffusion, we can see that there is a sum to the infinty involving the term \textit{m}, which is of course not computable since we do not have an infinit time/space ressources to compute. In computer science, we can consider stopping the computation of the sum if the steps are becoming smaller and smaller, to the point where it is not use anymore to continue computing steps of the sum.
\paragraph{}
In this case, we are dealing with numbers in a range of 100 to 300, so I considered that an error less than 10\textsuperscript{-4} is acceptable. To know when the \textit{m} value is to be stopped I printed out the temperature for the same time and space with a different m value. Time is t = 0.01,0.25,0.5 and space is x = 0.5.
\begin{table}[ht]
\centering
\begin{tabular}{c c c c}
\toprule
\textit{m} value & temperature at t = 0.5 & temperature at t = 0.25 & temperature at t = 0.01 \\
\midrule
1 &144.5380&101.0325&47.8530\\
5 &145.5377&110.1387&85.7275\\
10 &145.5377&110.1389&95.4361\\
20 &145.5377&110.1389&100.1123\\
49 &145.5377&110.1389&100.0000\\
50 &145.5377&110.1389&100.0000\\
100 &145.5377&110.1389&100.0000\\
\end{tabular}
\caption{Temperature by \textit{m} value}
\end{table}\\

The outcome of those results is that in this example, we don't need such a high value for \textit{m}, we can see that values stop varying after \textit{m} $>$ 49. A too small \textit{m} value is not accurate, at time \textit{t} = 0.01 the temperature is equal to 47\textsuperscript{o}F when it is supposed to be at least 100\textsuperscript{o}F. If we want an error less than 10\textsuperscript{-4}, \textit{m} = 50 is enough. Thanks to that, we know that we don't have to waste computer ressources and time on computing the analytical solution with a higher \textit{m} value.


\chapter{Laasonen method,\\ time step size and computation time}

\section{Theory}
\paragraph{Laasonen method}
The Laasonen method is a way of solving the heat diffusion equation by computing a result step by step. The Laasonen method is part of the implicit scheme meaning that each time step not only relies on the previous one but also one the current one. When computing the result, the previous time step has already been computed but the current one will need to be solved through a linear system. Let's see the equation to get a clearer idea:
\begin{equation}\frac{\partial T}{\partial t} = D\frac{\partial^2T }{\partial x^2}
\end{equation} 
\begin{equation}\frac{f\binom{n+1}{i} - f\binom{n}{i}}{\Delta t} = D\frac{f\binom{n+1}{i+1} - 2f\binom{n+1}{i}+ f\binom{n+1}{i-1}}{\Delta x^{2}}
\end{equation} 
\begin{equation}f\binom{n}{i} = -Cf\binom{n+1}{i+1}+(1+2C)f\binom{n+1}{i}-Cf\binom{n+1}{i-1}
   \text{ with }
C = D\frac{\Delta t}{\Delta x^{2}}
\end{equation}
\begin{equation}
\begin{pmatrix}
1+2C & -C & 0 & ... & 0\\
-C & 1+2C & -C& ... & 0\\
...&...&...&...&...\\
 0 & ...& -C &1+2C & -C \\
0 & ... &  0& -C & 1+2C
\end{pmatrix}
\begin{pmatrix}
f\binom{n+1}{0}\\
f\binom{n+1}{1}\\
...\\
...\\
f\binom{n+1}{k}
\end{pmatrix}
=
\begin{pmatrix}
f\binom{n}{0}\\
f\binom{n}{1}\\
...\\
...\\
f\binom{n}{k}
\end{pmatrix}
\end{equation}
With the above equations, we can see that we need to solve a linear system for each time step. Linear system can be really time and ressources consumming for a computer. That's why we need a good algorithm to resolve this linear system, if we take a look at the left matrix we can see that it is a diagonal matrix.\\
A linear system with a diagonal matrix can be solved with Thomas' algorithm, which is a really good linear system solving algorithm that works with diagonal matrix, it's time complexity is of:  \textit{O(2n)} with n being the size of the matrix, so the number of space step. L=1 $\Delta$t=0.05 so we have 21 space steps. Meaning that a matrix twice larger will "only" take four time the requiered time. ( A bit more explanation about Thomas' algorithm in the Appendix ).
\paragraph{}
Since the method relies on computing different time with a constant time step, a good interrogation would be: what is a good time step ? In theory, considering a stable method we can expect a more accurate solution the smaller the time step is.

\section{Pratice} 
Having smaller time steps allows us to have a better precision on each row, but if we decrease the time step we increase the number of linear system we need to solve. So I ran 4 simulations with various value of $\Delta$t, to compare the results. Let's see the norms of the errors matrices, the errors matrices is the Laasonen resulting matrix minus the analytical matrix:
\begin{table}[ht]
\centering
\begin{tabular}{c c c c c c}
\toprule
$\Delta$t&Number of time step&One norm&Two norm&uniform norm&computing time\\
\midrule
0.1 &5&50.26&67.01&154.56&0.14ms\\
0.05 &10&58.62&60.62&102.73&0.6ms\\
0.025 &20&67.90&52.13&64.04&0.42ms\\
0.01 &50&77.01&39.68&36.26&2.2ms\\
\end{tabular}
\caption{Error norms according to $\Delta$t}
\end{table} \\
Increasing the number of rows in the matrix by decreasing the time step size affects the one and two norms. Because the one norm is the highest error among the sums of error in each column and the two norm is just the sum of all errors. So more rows equals to possibly a worst one and two norms\\

\paragraph{one norm} The one norm is about errors in one column, so for $\Delta$t = 0.1, we can say that on average the "worst" column has an error of 50.26/5=10.05. Considering the fact that we are dealing with numbers in range of 100-300, this is almost a 10\% error on average by row. Compared to the $\Delta$t = 0.01 that has an average of 77.01/50 = 1.5 error, that's about 1\%error on average by row.\\
According to the one norm, decreazing the size of time steps is a good idea.

\paragraph{two norm} The two norm is just the square root of the sum of the square of errors, so increasing the numbers of rows in the matrix should increase the two norm, which is not the case. Between $\Delta$t = 0.1 and $\Delta$t = 0.05, we double the matrix size but the sum of errors is even smaller meaning that we have a lot less errors overall.

\paragraph{uniform norm} The uniform norm is the highest number among the sum of rows. Increasing the number of rows does not affect the behaviour of this number, so if this decreases with a smaller time step size, it means that the method is more and more accurate. By comparing the worst uniform norm 154.56 and the best 36.26,  we can say that the worst row with a $\Delta$t = 0.1 is nearly six times less accurate than the worst row with a $\Delta$t = 0.01.

\paragraph{} On a stable method such as Laasonen we have clearly seen that having a smaller $\Delta$t is giving the method a better accuracy, going from 10\% to nearly 1\% error with the first norm. In an ideal case we should find the best deal between time computing and errors, not to forget that If we want to be very accurate we would hurt ourself against errors implied by the numerical 64-bit system.

\chapter{Methods Result}
\section{Unstable Method}
As we said already, thoses numerical schemes are based on computing steps after steps using the principe of decomposing $\frac{\partial T}{\partial t}$ into something that looks like this $\frac{f\binom{n+1}{i} - f\binom{n-1}{i}}{2\Delta t}$. Doing this implies errors because in theory this is correct for the smallest $\Delta t$ possible . That is why, the more steps we have the more accurate results we have because the $\Delta$ in $f(x+\Delta x)$is smaller. This is introducing errors and we must be sure that the errors are not comulating. In order to do this, we study the stability of the methods on the given problem. This means that every method does not work on every problem. Methods can be stable under certains conditions mostly relying on a $\Delta$x-$\Delta$t ratio. They can also be unconditionally stable/unstable meaning that for any $\Delta$x-$\Delta$t, the method will (or will not) work at any case.
\paragraph{Richardson case}
Let's take a look at Richardson method, it consists in derivating centrally the time and space.
\begin{equation}\frac{\partial T}{\partial t} = D\frac{\partial^2T }{\partial x^2}
\end{equation} 
\begin{equation}\frac{f\binom{n+1}{i} - f\binom{n-1}{i}}{2\Delta t} = D\frac{f\binom{n}{i+1} - 2f\binom{n}{i}+ f\binom{n}{i}}{\Delta x^{2}}
\end{equation} 
\begin{equation}f\binom{n+1}{i} = Cf\binom{n}{i+1} - C2f\binom{n}{i}+ Cf\binom{n}{i-1})+f\binom{n-1}{i} \text{ with }
C = 2D\frac{\Delta t}{\Delta x^{2}}
\end{equation} 
Now that we have only one unknown we can figure it out. Since Richardson's method is unstable (see Appendix for proof) considering this problem, the results are not coherent at all.\\
\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{Richardson.png}
\caption{Richardson solution}
\end{figure}
\paragraph{}
I only plotted the three first steps because it shows that with only three steps we can clearly see that the methods is incoherent. Temperatures are supposed to go in a range of 100\textsuperscript{o}F to 300\textsuperscript{o}F. Which is clearly not the case here with only three time steps computed.

\section{Stable Methods}
Within the conditions defined in the original problem, Dufort-Frankel Laasonen and Crank-Nicholson methods are stable, which means that we do not have to fear for the errors to grow exponentially. We can even expected a better accuracy with time.
\paragraph{Mathematic} The derivation of the original equation into the equations used in those schemes can be found in the appendix.
\paragraph{results}
The best way to compare the results are to compare the norms of their error matrix. For that, we can take a look at table 3.1 that compares the one norm , two norm, uniform norm and the computation time of the three methods.\\
\begin{table}[ht]
\centering
\begin{tabular}{c c c c c}
\toprule
Method & One norm & Two norm & Uniform norm & Computation time \\
\midrule
Crank Nicolson & 14.95 & 14.48 & 20.77 & 2.04ms\\
Laasonen &77.01&39.68&36.26&3.17ms\\
Dufort-Frankel & 85.61 & 74.47 & 82.82 & 0.93ms\\
\end{tabular}
\caption{Accuracy of stable methods}
\end{table}\\
Looking at those results, we can clearly see the superiority of the implicit methods ( Crank-Nicolson \& Laasonen ) over the Dufort-Frankel method. The norms in this table represents the sum of errors, so the fewer errors the best a method is, regarding the considered norm. However, the computation time for Dufort-Frankel is far better. Crank-Nicolson seems to be the better deal here: the sum of the error of the least accurate time step is 20\textsuperscript{o}F. So considering 21 space step, that's a 1\textsuperscript{o}F error by number in average, which is acceptable. The second norm of Crank-Nicolson (14.48), that somehow represents the error in the all result matrix, is also the best.\\
The outcome of this table is that if we have enough time we should consider using Crank-Nicolson since it is the more accurate method but if if we don't have enough computation time we should consider using Dufort Frankel. Dufort Frankel is a bit less accurate but is way faster because its computation only requires solving one equation and not a linear system.

\chapter{Implementation}
Now that's we saw the theory and the results, let's take a look at the C++ implementation and the design of the application.
\paragraph{Class Diagram}
\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{ClassDiagram.png}
\caption{Class Diagramm}
\end{figure}

This is the class diagram of the application implementing the four schemes and the analytic solution.  The\textit{Solver} class uses \textit{Matrix} to store the solution of the problem where rows are time steps and columns are space steps. \textit{Solver} declares the virtual method \textit{computeSolution()} so that classes that inherit from it need to give an implementation of \textit{computeSolution()} if they wish to be instanciated in a \textit{main} programm.\\
The base class \textit{Solver} class holds all of the informations regarding the initial problem. Since it is attribut and not just hard-coded value in the code, all the child class are bounded to those values. Let's say that we want to see the solution with a surface temperature of 250\textsuperscript{o}F instead of 300\textsuperscript{o}F, we just have to instanciate the object with a different \textit{Tsur} value.
\paragraph{functions}
To print out the solution both in screen or in a file, I added the redefinition of the $"<<"$ operator. On screen, the matrix will print the solution value for time [0.1, 0.2, 0.3, 0.4, 0.5] and will write on a file in a way that would be exploitable by GNUPlot.
I added the minus operator in the matrix class to be able to substract matrices in order to get the error matrix. \\When instanciated an object of a Solver child class, you have to respect some rules in order not to get exceptions. For example, it is impossible to have a \textit{dx} larger than a \textit{L} it would not make any sense, so the programm would raise an \textit{invalid\_argument} exception

\chapter*{Conclusion}
We had a problem that requiered a numerical approximation and four methods that would provide differents solutions with differents computation times. As we have seen the first thing to look at, is wether the method is stable or not to be sure to get "correct results". An other important aspect is the accuracy that we want to have, often the more accurate is a method, the more time consuming it will be, so we should look at the complexity of the method to know wether we can compute it in a reasonable time. Because all of the methods are numerical approch there will always be errors either implied by the method or by the limit of the machine, not being able to store the correct value of a number.\\
In the scope of the valid solutions, there is not a better solution above the others for all cases, it all depends on the computation time and on the accuracy wanted.
\begin{appendix}
\clearpage
\phantomsection
\chapter{Mathematic derivation}
\paragraph{Richardson}
\begin{equation}\frac{\partial T}{\partial t} = D\frac{\partial^2T }{\partial x^2}
\end{equation} 
\begin{equation}\frac{f\binom{n+1}{i} - f\binom{n-1}{i}}{2\Delta t} = D\frac{f\binom{n}{i+1} - 2f\binom{n}{i}+ f\binom{n}{i}}{\Delta x^{2}}
\end{equation} 
\begin{equation}f\binom{n+1}{i} = Cf\binom{n}{i+1} - C2f\binom{n}{i}+ Cf\binom{n}{i-1})+f\binom{n-1}{i} \text{ with }
C = 2D\frac{\Delta t}{\Delta x^{2}}
\end{equation} 
\paragraph{Dufort-Frankel}
\begin{equation}\frac{\partial T}{\partial t} = D\frac{\partial^2T }{\partial x^2}
\end{equation} 
\begin{equation}\frac{f\binom{n+1}{i} - f\binom{n-1}{i}}{2\Delta t} = D\frac{f\binom{n}{i+1} - (f\binom{n+1}{i}+f\binom{n-1}{i})+f\binom{n}{i-1}}{\Delta x^{2}}
\end{equation} 
\begin{equation}
f\binom{n+1}{i} = \frac{f\binom{n-1}{i}+2C(f\binom{n}{i+1}-f\binom{n-1}{i}+f\binom{n}{i-1})}{(1+2C)}\text{ with }
C = 2D\frac{\Delta t}{\Delta x^{2}}
\end{equation} 
\paragraph{Laasonen}
\begin{equation}\frac{\partial T}{\partial t} = D\frac{\partial^2T }{\partial x^2}
\end{equation} 
\begin{equation}\frac{f\binom{n+1}{i} - f\binom{n}{i}}{\Delta t} = D\frac{f\binom{n+1}{i+1} - 2f\binom{n+1}{i}+ f\binom{n+1}{i-1}}{\Delta x^{2}}
\end{equation} 
\begin{equation}f\binom{n}{i} = -Cf\binom{n+1}{i+1}+(1+2C)f\binom{n+1}{i}-Cf\binom{n+1}{i-1}
   \text{ with }
C = D\frac{\Delta t}{\Delta x^{2}}
\end{equation}
\begin{equation}
\begin{pmatrix}
1+2C & -C & 0 & ... & 0\\
-C & 1+2C & -C& ... & 0\\
...&...&...&...&...\\
 0 & ...& -C &1+2C & -C \\
0 & ... &  0& -C & 1+2C
\end{pmatrix}
\begin{pmatrix}
f\binom{n+1}{0}\\
f\binom{n+1}{1}\\
...\\
...\\
f\binom{n+1}{k}
\end{pmatrix}
=
\begin{pmatrix}
f\binom{n}{0}\\
f\binom{n}{1}\\
...\\
...\\
f\binom{n}{k}
\end{pmatrix}
\end{equation}
\paragraph{Crank Nicolson}
\begin{equation}\frac{\partial T}{\partial t} = D\frac{\partial^2T }{\partial x^2}
\end{equation} 
\begin{equation}\frac{f\binom{n+1}{i} - f\binom{n}{i}}{\Delta t} = D\frac{f\binom{n}{i+1} - 2f\binom{n}{i}+ f\binom{n}{i-1}}{2\Delta x^{2}}+D\frac{f\binom{n+1}{i+1} - 2f\binom{n+1}{i}+ f\binom{n+1}{i-1}}{2\Delta x^{2}}
\end{equation} 
\begin{equation}-Cf\binom{n+1}{i+1}+(1+2C)f\binom{n+1}{i}-Cf\binom{n+1}{i-1} = Cf\binom{n}{i+1}+(1-2C)f\binom{n}{i}+Cf\binom{n}{i-1}
\end{equation}
\begin{equation}
\begin{pmatrix}
1+2C & -C & 0 & ... & 0\\
-C & 1+2C & -C& ... & 0\\
...&...&...&...&...\\
 0 & ...& -C &1+2C & -C \\
0 & ... &  0& -C & 1+2C
\end{pmatrix}
\begin{pmatrix}
f\binom{n+1}{0}\\
f\binom{n+1}{1}\\
...\\
...\\
f\binom{n+1}{k}
\end{pmatrix}
= 
\end{equation}
\begin{equation}
\begin{pmatrix}
Cf\binom{n}{1}+(1-2C)f\binom{n}{0}+C*Tsurface\\
Cf\binom{n}{2}+(1-2C)f\binom{n}{1}+Cf\binom{n}{0}\\
...\\
...\\
C*Tsurface+(1-2C)f\binom{n}{k}+Cf\binom{n}{k-1}
\end{pmatrix}
   \text{ with }
C = D\frac{\Delta t}{\Delta x^{2}}
\end{equation}

\chapter{Thomas' Algorithm}
Thomas' algorithm is an algorithm capable of solving linear system that can be stored in a diagonal matrix.
If we take an example: \\$\begin{pmatrix}
b & c & ... &&& 0\\
a & b & c & ...&... & 0\\
&&&&&\\
&&&&&\\
 0 & &...& a&b & c \\
0 &  &&...& a & b
\end{pmatrix}
\begin{pmatrix}
x_{0}\\
x_{1}\\
...\\
...\\
...\\
x_{k}
\end{pmatrix}
=
\begin{pmatrix}
d_{0}\\
d_{1}\\
...\\
...\\
...\\
d_{k}
\end{pmatrix}$\\
then we changed the matrix to look like this, by substracting rows, having $c_{i}' = \frac{c_{i}}{b_{i}-a_{i}c_{i-1}'}$: \\$\begin{pmatrix}
1 & c' & ... &&& 0\\
0 & 1 & c' & ...&... & 0\\
&&&&&\\
&&&&&\\
 0 & &...& 0&1 & c \\
0 &  &&...& 0 & 1
\end{pmatrix}
\begin{pmatrix}
x_{0}\\
x_{1}\\
...\\
...\\
...\\
x_{k}
\end{pmatrix}
=
\begin{pmatrix}
d_{0}'\\
d_{1}'\\
...\\
...\\
...\\
d_{k}'
\end{pmatrix}$\\
At this point, we have a one unknown by equation, so the method is just working backward on the equation with $x_{i} = d_{i}'x_{i+1}$\\
\paragraph{Complexity}About the complexity, if there is \textit{N} equations then the matrix will be of size \textit{N}. It's \textit{N} operations to transform the matrix and \textit{N} operations again to go backward. So the complexity is of \textit{O(2N)} 
\end{appendix}
\chapter{Documentation}
\section{documentation} Here you can see the latex-pdf documentation generated by Doxygen. With the source code, there is the html documentation that you might find more attractive.
\includepdf[pages=1-42]{refman.pdf}

\end{document}